(window.webpackJsonp=window.webpackJsonp||[]).push([[102],{485:function(s,a,t){"use strict";t.r(a);var e=t(11),r=Object(e.a)({},(function(){var s=this,a=s._self._c;return a("ContentSlotsDistributor",{attrs:{"slot-key":s.$parent.slotKey}},[a("h2",{attrs:{id:"面试题"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#面试题"}},[s._v("#")]),s._v(" 面试题")]),s._v(" "),a("p",[s._v("有了解过 Redis rehash 的过程吗？")]),s._v(" "),a("h2",{attrs:{id:"面试官心理分析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#面试官心理分析"}},[s._v("#")]),s._v(" 面试官心理分析")]),s._v(" "),a("p",[s._v("这个知识点算 redis 中比较低频的面试点，但是当你在介绍 HashMap 的 rehash 或者 ConcurrentHashMap 的 rehash 过程中，可以主动和面试官提及你不仅了解这些，同时还了解 Redis 中的 rehash 过程。")]),s._v(" "),a("p",[s._v("Redis 是以速度快，性能好著称的，我们知道 Redis 一开始的容量是有限的，当容量不足时，需要扩容，那扩容的方式是什么？一次性全部将数据转移吗？那当数据量上千万上亿，这必定会阻塞 Redis 对命令的执行。因此就非常有必要了解一下 Redis 中的 rehash 过程。")]),s._v(" "),a("h2",{attrs:{id:"面试题剖析"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#面试题剖析"}},[s._v("#")]),s._v(" 面试题剖析")]),s._v(" "),a("p",[s._v("众所周知，Redis 主要用于存储键值对("),a("code",[s._v("Key-Value Pair")]),s._v(")，而键值对的存储方式是由字典实现，而 Redis 中字典的底层又是通过哈希表来实现的。通过哈希表中的节点保存字典中的键值对。类似 Java 中的 HashMap，将 Key 通过哈希函数映射到哈希表节点位置。")]),s._v(" "),a("p",[s._v("Redis 中字典的数据结构如下：")]),s._v(" "),a("div",{staticClass:"language-c line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-c"}},[a("code",[a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 字典对应的数据结构，有关hash表的结构可以参考redis源码，再次就不进行描述")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("typedef")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("struct")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("dict")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("{")]),s._v("\n    dictType "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("type"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 字典类型")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("void")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v("privdata"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("  "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 私有数据")]),s._v("\n    dictht ht"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// 2个哈希表，这也是进行rehash的重要数据结构，从这也看出字典的底层通过哈希表进行实现。")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("long")]),s._v(" rehashidx"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("// rehash过程的重要标志，值为-1表示rehash未进行")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("int")]),s._v(" iterators"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("   "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("//  当前正在迭代的迭代器数")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("}")]),s._v(" dict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(";")]),s._v("\n")])]),s._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[s._v("1")]),a("br"),a("span",{staticClass:"line-number"},[s._v("2")]),a("br"),a("span",{staticClass:"line-number"},[s._v("3")]),a("br"),a("span",{staticClass:"line-number"},[s._v("4")]),a("br"),a("span",{staticClass:"line-number"},[s._v("5")]),a("br"),a("span",{staticClass:"line-number"},[s._v("6")]),a("br"),a("span",{staticClass:"line-number"},[s._v("7")]),a("br"),a("span",{staticClass:"line-number"},[s._v("8")]),a("br")])]),a("p",[s._v("在对哈希表进行扩展或者收缩操作时，程序需要将现有哈希表包含的所有键值对 rehash 到新哈希表里面，具体过程如下：")]),s._v(" "),a("h3",{attrs:{id:"_1-为字典的备用哈希表分配空间。"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-为字典的备用哈希表分配空间。"}},[s._v("#")]),s._v(" 1. 为字典的备用哈希表分配空间。")]),s._v(" "),a("p",[s._v('如果执行的是扩展操作，那么备用哈希表的大小为第一个大于等于需要扩容的哈希表的键值对数量*2 的 2"(2 的 n 次方幂);【'),a("code",[s._v("5*2=10,")]),s._v('所以备用哈希表的容量为第一个大于 10 的 2"，即 16】')]),s._v(" "),a("p",[s._v("如果执行的是收缩操作,那么备用哈希表的大小为第一个大于等于需要扩容的哈希表的键值对数量（"),a("code",[s._v("ht[0] .used")]),s._v('）的 2"。')]),s._v(" "),a("h3",{attrs:{id:"_2-渐进式-rehash"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-渐进式-rehash"}},[s._v("#")]),s._v(" 2. 渐进式 rehash")]),s._v(" "),a("p",[s._v("rehash 过程在数据量非常大（几千万、亿）的情况下并不是一次性地完成的，而是"),a("strong",[s._v("渐进式地")]),s._v("完成的。"),a("strong",[s._v("渐进式 rehash")]),s._v("的好处在于避免对服务器造成影响。")]),s._v(" "),a("p",[s._v("渐进式 rehash 的本质：")]),s._v(" "),a("ol",[a("li",[s._v("借助 rehashidx，将 rehash 键值对所需的计算工作均摊到对字典的每个添加、删除、查找和更新操作上，从而避免了集中式 rehash 而带来的庞大计算量。")]),s._v(" "),a("li",[s._v("在 rehash 进行期间，每次对字典执行添加、删除、查找或者更新操作时，程序除了执行指定的操作以外，还会顺带将原哈希表在 rehashidx 索引上的所有键值对 rehash 到备用哈希表，当 rehash 工作完成之后，程序将 rehashidx 属性的值加 1。")])])])}),[],!1,null,null,null);a.default=r.exports}}]);